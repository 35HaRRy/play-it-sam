{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79b5811c-1074-495f-9722-8325b5e717d3",
      "metadata": {},
      "source": [
        "# Plan-and-Execute\n",
        "\n",
        "This notebook shows how to create a \"plan-and-execute\" style agent. This is heavily inspired by the [Plan-and-Solve](https://arxiv.org/abs/2305.04091) paper as well as the [Baby-AGI](https://github.com/yoheinakajima/babyagi) project.\n",
        "\n",
        "The core idea is to first come up with a multi-step plan, and then go through that plan one item at a time.\n",
        "After accomplishing a particular task, you can then revisit the plan and modify as appropriate.\n",
        "\n",
        "\n",
        "The general computational graph looks like the following:\n",
        "\n",
        "\n",
        "![plan-and-execute diagram](../img/plan-and-execute.png)\n",
        "\n",
        "\n",
        "This compares to a typical [ReAct](https://arxiv.org/abs/2210.03629) style agent where you think one step at a time.\n",
        "The advantages of this \"plan-and-execute\" style agent are:\n",
        "\n",
        "1. Explicit long term planning (which even really strong LLMs can struggle with)\n",
        "2. Ability to use smaller/weaker models for the execution step, only using larger/better models for the planning step\n",
        "\n",
        "\n",
        "The following walkthrough demonstrates how to do so in LangGraph. The resulting agent will leave a trace like the following example: ([link](https://smith.langchain.com/public/d46e24d3-dda6-44d5-9550-b618fca4e0d4/r))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44a72d6-7e0c-4478-9d20-4c09000420a8",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, we need to install the packages required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b451b58a-89bd-424f-8c06-0d9fe325e01b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: C:\\Users\\hayri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain-community langchain_groq tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35f267b0-98db-4a59-8b2c-a23f795576ff",
      "metadata": {},
      "source": [
        "Next, we need to set API keys for Groq (the LLM we will use) and Tavily (the search tool we will use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ce438281-08d5-4804-afe7-e4089f7b016b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"GROQ_API_KEY\")\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2d7981-3737-4134-8bef-d00d18d4e91d",
      "metadata": {},
      "source": [
        "Optionally, we can set API key for LangSmith tracing, which will give us best-in-class observability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "01f460d1-f26f-47d1-ae76-de74d5d851de",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "_set_env(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Plan-and-execute\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5fb09a-0311-44c2-b243-d0e80de78902",
      "metadata": {},
      "source": [
        "## Define Tools\n",
        "\n",
        "We will first define the tools we want to use. For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation [here](https://python.langchain.com/v0.2/docs/how_to/custom_tools) on how to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "25b9ec62-0675-4715-811c-9b32c635b22f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tools = [TavilySearchResults(max_results=3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dcda478-fa80-4e3e-bb35-0f622fe73a31",
      "metadata": {},
      "source": [
        "## Define our Execution Agent\n",
        "\n",
        "Now we will create the execution agent we want to use to execute tasks. \n",
        "Note that for this example, we will be using the same execution agent for each task, but this doesn't HAVE to be the case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "72d233ca-1dbf-4b43-b680-b3bf39e3691f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hayri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "You are a helpful assistant.\n",
            "\n",
            "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
            "\n",
            "\u001b[33;1m\u001b[1;3m{{messages}}\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hayri\\AppData\\Local\\Temp\\ipykernel_13032\\215378746.py:12: LangGraphDeprecationWarning: Parameter 'messages_modifier' in function 'create_react_agent' is deprecated as of version 0.1.9 and will be removed in version 0.3.0. Use 'state_modifier' parameter instead.\n",
            "  agent_executor = create_react_agent(llm, tools, messages_modifier=prompt)\n"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"wfh/react-agent-executor\")\n",
        "prompt.pretty_print()\n",
        "\n",
        "# Choose the LLM that will drive the agent\n",
        "llm = ChatGroq(model_name=\"llama-3.1-70b-versatile\", temperature=0.0)\n",
        "agent_executor = create_react_agent(llm, tools, messages_modifier=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "746e697a-dec4-4342-a814-9b3456828169",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='who is the winner of the us open', id='4f595ab1-c7aa-4639-a67a-b7d911e76679'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gj9m', 'function': {'arguments': '{\"query\": \"US Open winner\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 292, 'total_tokens': 312, 'completion_time': 0.08, 'prompt_time': 0.084212109, 'queue_time': 0.207806867, 'total_time': 0.164212109}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-de6714c5-d7ef-404e-8bb9-d136f014634c-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'US Open winner'}, 'id': 'call_gj9m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 292, 'output_tokens': 20, 'total_tokens': 312}),\n",
              "  ToolMessage(content='[{\"url\": \"https://apnews.com/article/us-open-final-live-updates-djokovic-medvedev-8a4a26f8d77ef9ab2fb3efe1096dce7e\", \"content\": \"Novak Djokovic wins the US Open for his 24th Grand Slam title by beating Daniil Medvedev\\\\nNovak Djokovic, of Serbia, holds up the championship trophy after defeating Daniil Medvedev, of Russia, in the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. (AP Photo/Manu Fernandez)\\\\nDaniil Medvedev, of Russia, sits on the court after a rally against Novak Djokovic, of Serbia, during the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. (AP Photo/Manu Fernandez)\\\\nDaniil Medvedev, of Russia, sits on the court after a rally against Novak Djokovic, of Serbia, during the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. (AP Photo/Manu Fernandez)\\\\nDaniil Medvedev, of Russia, sits on the court after a rally against Novak Djokovic, of Serbia, during the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. Novak Djokovic, of Serbia, reveals a t-shirt honoring the number 24 and Kobe Bryant after defeating Daniil Medvedev, of Russia, in the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York.\"}, {\"url\": \"https://en.wikipedia.org/wiki/List_of_U.S._Open_(golf)_champions\", \"content\": \"U.S. Open champions are automatically invited to play in the other three majors (the Masters, the Open Championship (British Open), and the PGA Championship) for the next five years, and earn a ten-year exemption from qualifying for the U.S. Open. Eight others have led wire-to-wire in nine tournaments if ties after a round are counted: Willie Anderson in 1903, Alex Smith in 1906, Chick Evans in 1916, Tommy Bolt in 1958, Nicklaus in 1972 and 1980, Hubert Green in 1977, Payne Stewart in 1991, and Retief Goosen in 2001.[7]\\\\nChampions[edit]\\\\nBy year[edit]\\\\nMultiple champions[edit]\\\\nChampions by nationality[edit]\\\\nNotes[edit]\\\\nReferences[edit]\\\\nExternal links[edit] The champion receives a gold champion\\'s medal, and the U.S. Open Championship Cup, which the winner is allowed to keep for a year.[3]\\\\nWillie Anderson, Bobby Jones, Ben Hogan and Jack Nicklaus hold the record for the most U.S. Open victories, with four victories each.[4] [6]\\\\nThe U.S. Open has been won wire-to-wire by seven golfers on eight occasions: 1914 by Walter Hagen, 1921 by Jim Barnes, 1953 by Hogan, 1970 by Tony Jacklin, 2000 and 2002 by Tiger Woods, 2011 by McIlroy, and 2014 by Martin Kaymer.[7][8] They also receive membership on the PGA Tour for the following five seasons and invitations to The Players Championship for the five years following their victories.[2]\"}, {\"url\": \"https://apnews.com/article/carlos-alcaraz-us-open-tennis-championships-casper-ruud-e3d006754469c6f518bc46a1599914a8\", \"content\": \"Carlos Alcaraz wins US Open for 1st Slam title, top ranking\\\\nCarlos Alcaraz, of Spain, holds up the championship trophy after defeating Casper Ruud, of Norway, in the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Matt Rourke)\\\\nCasper Ruud, of Norway, holds up the runner-up trophy after losing to Carlos Alcaraz, of Spain, in the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Matt Rourke)\\\\nCarlos Alcaraz, of Spain, holds up the championship trophy after defeating Casper Ruud, of Norway, in the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Mary Altaffer)\\\\nCarlos Alcaraz, of Spain, kisses the championship trophy after defeating Casper Ruud, of Norway, in the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Mary Altaffer)\\\\nCarlos Alcaraz, of Spain, kisses the championship trophy after defeating Casper Ruud, of Norway, in the men\\\\u2019s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York.\"}]', name='tavily_search_results_json', id='3f61513f-a53a-42af-be1c-78c4865b15e2', tool_call_id='call_gj9m', artifact={'query': 'US Open winner', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"US Open men's final: Novak Djokovic beats Daniil Medvedev and wins 24th ...\", 'url': 'https://apnews.com/article/us-open-final-live-updates-djokovic-medvedev-8a4a26f8d77ef9ab2fb3efe1096dce7e', 'content': 'Novak Djokovic wins the US Open for his 24th Grand Slam title by beating Daniil Medvedev\\nNovak Djokovic, of Serbia, holds up the championship trophy after defeating Daniil Medvedev, of Russia, in the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. (AP Photo/Manu Fernandez)\\nDaniil Medvedev, of Russia, sits on the court after a rally against Novak Djokovic, of Serbia, during the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. (AP Photo/Manu Fernandez)\\nDaniil Medvedev, of Russia, sits on the court after a rally against Novak Djokovic, of Serbia, during the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. (AP Photo/Manu Fernandez)\\nDaniil Medvedev, of Russia, sits on the court after a rally against Novak Djokovic, of Serbia, during the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York. Novak Djokovic, of Serbia, reveals a t-shirt honoring the number 24 and Kobe Bryant after defeating Daniil Medvedev, of Russia, in the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 10, 2023, in New York.', 'score': 0.99693656, 'raw_content': None}, {'title': 'List of U.S. Open (golf) champions - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/List_of_U.S._Open_(golf)_champions', 'content': \"U.S. Open champions are automatically invited to play in the other three majors (the Masters, the Open Championship (British Open), and the PGA Championship) for the next five years, and earn a ten-year exemption from qualifying for the U.S. Open. Eight others have led wire-to-wire in nine tournaments if ties after a round are counted: Willie Anderson in 1903, Alex Smith in 1906, Chick Evans in 1916, Tommy Bolt in 1958, Nicklaus in 1972 and 1980, Hubert Green in 1977, Payne Stewart in 1991, and Retief Goosen in 2001.[7]\\nChampions[edit]\\nBy year[edit]\\nMultiple champions[edit]\\nChampions by nationality[edit]\\nNotes[edit]\\nReferences[edit]\\nExternal links[edit] The champion receives a gold champion's medal, and the U.S. Open Championship Cup, which the winner is allowed to keep for a year.[3]\\nWillie Anderson, Bobby Jones, Ben Hogan and Jack Nicklaus hold the record for the most U.S. Open victories, with four victories each.[4] [6]\\nThe U.S. Open has been won wire-to-wire by seven golfers on eight occasions: 1914 by Walter Hagen, 1921 by Jim Barnes, 1953 by Hogan, 1970 by Tony Jacklin, 2000 and 2002 by Tiger Woods, 2011 by McIlroy, and 2014 by Martin Kaymer.[7][8] They also receive membership on the PGA Tour for the following five seasons and invitations to The Players Championship for the five years following their victories.[2]\", 'score': 0.9921233, 'raw_content': None}, {'title': 'Carlos Alcaraz wins US Open for 1st Slam title, top ranking', 'url': 'https://apnews.com/article/carlos-alcaraz-us-open-tennis-championships-casper-ruud-e3d006754469c6f518bc46a1599914a8', 'content': 'Carlos Alcaraz wins US Open for 1st Slam title, top ranking\\nCarlos Alcaraz, of Spain, holds up the championship trophy after defeating Casper Ruud, of Norway, in the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Matt Rourke)\\nCasper Ruud, of Norway, holds up the runner-up trophy after losing to Carlos Alcaraz, of Spain, in the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Matt Rourke)\\nCarlos Alcaraz, of Spain, holds up the championship trophy after defeating Casper Ruud, of Norway, in the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Mary Altaffer)\\nCarlos Alcaraz, of Spain, kisses the championship trophy after defeating Casper Ruud, of Norway, in the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York. (AP Photo/Mary Altaffer)\\nCarlos Alcaraz, of Spain, kisses the championship trophy after defeating Casper Ruud, of Norway, in the men’s singles final of the U.S. Open tennis championships, Sunday, Sept. 11, 2022, in New York.', 'score': 0.99036634, 'raw_content': None}], 'response_time': 2.04}),\n",
              "  AIMessage(content='The winner of the US Open is Novak Djokovic.', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 1496, 'total_tokens': 1510, 'completion_time': 0.056, 'prompt_time': 0.515767799, 'queue_time': 0.005063397999999997, 'total_time': 0.571767799}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run-e44421e5-7208-4d36-831e-783afdd3b578-0', usage_metadata={'input_tokens': 1496, 'output_tokens': 14, 'total_tokens': 1510})]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"messages\": [(\"user\", \"who is the winner of the us open\")]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf66804-44b2-4904-b1a7-17ad70b551f5",
      "metadata": {},
      "source": [
        "## Define the State\n",
        "\n",
        "Let's now start by defining the state the track for this agent.\n",
        "\n",
        "First, we will need to track the current plan. Let's represent that as a list of strings.\n",
        "\n",
        "Next, we should track previously executed steps. Let's represent that as a list of tuples (these tuples will contain the step and then the result)\n",
        "\n",
        "Finally, we need to have some state to represent the final response as well as the original input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8eeeaeea-8f10-4fbe-8e24-4e1a2381a009",
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated, List, Tuple, TypedDict\n",
        "\n",
        "\n",
        "class PlanExecute(TypedDict):\n",
        "    input: str\n",
        "    plan: List[str]\n",
        "    past_steps: Annotated[List[Tuple], operator.add]\n",
        "    response: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dbd770a-9941-40a9-977e-4d55359eee21",
      "metadata": {},
      "source": [
        "## Planning Step\n",
        "\n",
        "Let's now think about creating the planning step. This will use function calling to create a plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4a88626d-6dfd-4488-87f0-a9a0dd6da44c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    \"\"\"Plan to follow in future\"\"\"\n",
        "\n",
        "    steps: List[str] = Field(\n",
        "        description=\"different steps to follow, should be in sorted order\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ec7b1867-1ea3-4df3-9a98-992a1c32ec49",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "planner_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "planner = planner_prompt | ChatGroq(model_name=\"llama-3.1-70b-versatile\", temperature=0.0).with_structured_output(Plan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "67ce37b7-e089-479b-bcb8-c3f5d9874613",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Plan(steps=['Find the current Australia Open winner', 'Find the hometown of the current Australia Open winner'])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "planner.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\"user\", \"what is the hometown of the current Australia open winner?\")\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e09ad9d-6f90-4bdc-bb43-b1ce94517c29",
      "metadata": {},
      "source": [
        "## Re-Plan Step\n",
        "\n",
        "Now, let's create a step that re-does the plan based on the result of the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ec2d12cc-016a-44d1-aa08-4c5ce1e8fe2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "\n",
        "class Response(BaseModel):\n",
        "    \"\"\"Response to user.\"\"\"\n",
        "\n",
        "    response: str\n",
        "\n",
        "\n",
        "class Act(BaseModel):\n",
        "    \"\"\"Action to perform.\"\"\"\n",
        "\n",
        "    action: Union[Response, Plan] = Field(\n",
        "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
        "        \"If you need to further use tools to get the answer, use Plan.\"\n",
        "    )\n",
        "\n",
        "\n",
        "replanner_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
        "\n",
        "Your objective was this:\n",
        "{input}\n",
        "\n",
        "Your original plan was this:\n",
        "{plan}\n",
        "\n",
        "You have currently done the follow steps:\n",
        "{past_steps}\n",
        "\n",
        "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "replanner = replanner_prompt | ChatGroq(model_name=\"llama-3.1-70b-versatile\", temperature=0.0).with_structured_output(Act)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859abd13-6ba0-45ad-b341-e652dd5f755b",
      "metadata": {},
      "source": [
        "## Create the Graph\n",
        "\n",
        "We can now create the graph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6c8e0dad-bcea-4c9a-8922-0d820892e2d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "\n",
        "async def execute_step(state: PlanExecute):\n",
        "    plan = state[\"plan\"]\n",
        "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
        "    task = plan[0]\n",
        "    task_formatted = f\"\"\"For the following plan:\n",
        "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
        "    agent_response = await agent_executor.ainvoke(\n",
        "        {\"messages\": [(\"user\", task_formatted)]}\n",
        "    )\n",
        "    return {\n",
        "        # \"past_steps\": (task, agent_response[\"messages\"][-1].content),\n",
        "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
        "    }\n",
        "\n",
        "\n",
        "async def plan_step(state: PlanExecute):\n",
        "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
        "    return {\"plan\": plan.steps}\n",
        "\n",
        "\n",
        "async def replan_step(state: PlanExecute):\n",
        "    output = await replanner.ainvoke(state)\n",
        "    if isinstance(output.action, Response):\n",
        "        return {\"response\": output.action.response}\n",
        "    else:\n",
        "        return {\"plan\": output.action.steps}\n",
        "\n",
        "\n",
        "def should_end(state: PlanExecute) -> Literal[\"agent\", \"__end__\"]:\n",
        "    if \"response\" in state and state[\"response\"]:\n",
        "        return \"__end__\"\n",
        "    if \"plan\" in state and state[\"plan\"] and len(state[\"plan\"]) == 0:\n",
        "        return \"__end__\"\n",
        "    else:\n",
        "        return \"agent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e954cea0-5ccc-46c2-a27b-f5b7185b597d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START\n",
        "\n",
        "workflow = StateGraph(PlanExecute)\n",
        "\n",
        "# Add the plan node\n",
        "workflow.add_node(\"planner\", plan_step)\n",
        "\n",
        "# Add the execution step\n",
        "workflow.add_node(\"agent\", execute_step)\n",
        "\n",
        "# Add a replan node\n",
        "workflow.add_node(\"replan\", replan_step)\n",
        "\n",
        "workflow.add_edge(START, \"planner\")\n",
        "\n",
        "# From plan we go to agent\n",
        "workflow.add_edge(\"planner\", \"agent\")\n",
        "\n",
        "# From agent, we replan\n",
        "workflow.add_edge(\"agent\", \"replan\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"replan\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_end,\n",
        ")\n",
        "\n",
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7363e528",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b8ac1f67-e87a-427c-b4f7-44351295b788",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AsyncCallbackManager.merge(): Parent run IDs do not match. Using the parent run ID of the first callback manager.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'plan': ['Search for the 2024 Australia Open winner', 'Find the hometown of the winner']}\n",
            "{'past_steps': [('Search for the 2024 Australia Open winner', 'The winner of the 2024 Australia Open is Jannik Sinner.')]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AsyncCallbackManager.merge(): Parent run IDs do not match. Using the parent run ID of the first callback manager.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'plan': ['Find the hometown of Jannik Sinner']}\n",
            "{'past_steps': [('Find the hometown of Jannik Sinner', \"Jannik Sinner's hometown is Sexten (also known as Sesto) in northern Italy.\")]}\n"
          ]
        },
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{\"action\": {\"response\": \"Jannik Sinner\"}}\\'s hometown is Sexten (also known as Sesto) in northern Italy.</function>'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m}\n\u001b[0;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the hometown of the 2024 Australia open winner?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream(inputs, config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langgraph\\pregel\\__init__.py:1265\u001b[0m, in \u001b[0;36mPregel.astream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m-> 1265\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeoutError\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langgraph\\pregel\\__init__.py:1419\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[0;32m   1417\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langgraph\\pregel\\executor.py:123\u001b[0m, in \u001b[0;36mAsyncBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: asyncio\u001b[38;5;241m.\u001b[39mTask) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langgraph\\pregel\\retry.py:75\u001b[0m, in \u001b[0;36marun_with_retry\u001b[1;34m(task, retry_policy, stream)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\runnables\\base.py:2920\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2918\u001b[0m     part \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(step\u001b[38;5;241m.\u001b[39mainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   2919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[1;32m-> 2920\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2922\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langgraph\\utils.py:124\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[25], line 25\u001b[0m, in \u001b[0;36mreplan_step\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplan_step\u001b[39m(state: PlanExecute):\n\u001b[1;32m---> 25\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m replanner\u001b[38;5;241m.\u001b[39mainvoke(state)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output\u001b[38;5;241m.\u001b[39maction, Response):\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: output\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mresponse}\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\runnables\\base.py:2920\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2918\u001b[0m     part \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(step\u001b[38;5;241m.\u001b[39mainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   2919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[1;32m-> 2920\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2922\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\runnables\\base.py:5106\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5100\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[0;32m   5101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5102\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5103\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[0;32m   5107\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5109\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5110\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:297\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    295\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    296\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m--> 297\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[0;32m    298\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    299\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    300\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    301\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    302\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    303\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    304\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    306\u001b[0m     )\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[0;32m    787\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    788\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:746\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    734\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    736\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    744\u001b[0m             ]\n\u001b[0;32m    745\u001b[0m         )\n\u001b[1;32m--> 746\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    747\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    748\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    750\u001b[0m ]\n\u001b[0;32m    751\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:922\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 922\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[0;32m    923\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    924\u001b[0m         )\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    926\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_groq\\chat_models.py:493\u001b[0m, in \u001b[0;36mChatGroq._agenerate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    489\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    492\u001b[0m }\n\u001b[1;32m--> 493\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\groq\\resources\\chat\\completions.py:578\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    467\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    580\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m    581\u001b[0m             {\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    603\u001b[0m             },\n\u001b[0;32m    604\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    605\u001b[0m         ),\n\u001b[0;32m    606\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    607\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    608\u001b[0m         ),\n\u001b[0;32m    609\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    610\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    611\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m    612\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\groq\\_base_client.py:1762\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1750\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1757\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m   1759\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1760\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\groq\\_base_client.py:1478\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1471\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1477\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1479\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1480\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1481\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1482\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1483\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m   1484\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\groq\\_base_client.py:1569\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1566\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m   1568\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1572\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1573\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1576\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1577\u001b[0m )\n",
            "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{\"action\": {\"response\": \"Jannik Sinner\"}}\\'s hometown is Sexten (also known as Sesto) in northern Italy.</function>'}}"
          ]
        }
      ],
      "source": [
        "config = {\"recursion_limit\": 50}\n",
        "inputs = {\"input\": \"what is the hometown of the 2024 Australia open winner?\"}\n",
        "async for event in app.astream(inputs, config=config):\n",
        "    for k, v in event.items():\n",
        "        if k != \"__end__\":\n",
        "            print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf585a9-0f1e-4910-bd00-65e7bb05b6e6",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congrats on making a plan-and-execute agent! One known limitations of the above design is that each task is still executed in sequence, meaning embarrassingly parallel operations all add to the total execution time. You could improve on this by having each task represented as a DAG (similar to LLMCompiler), rather than a regular list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8f7955-2cc9-4ebb-8c41-13abb3351a24",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
